# assign residuals and fitted points to variables
model_res = resid(step.model)
fitted <- fitted(step.model)
# summary(model_res)
#%>% kbl(caption = "Residual Summary") %>%
#  kable_classic(full_width = F, html_font = "Cambria")
# first view loks fine in terms of normality
hist(model_res, main="Histogram of Residuals",
ylab="Residuals")
# plot residuals vs fitted set
plot(fitted, model_res, ylab="Residuals", xlab="BMI Score Fitted", main="BMI Prediction (residuals)")
#Q-Q Plot of residuals
qqnorm(model_res)
qqline(model_res)
# perform dwtest
dwtest(step.model) #Test for independence of residuals
library(car)
# perform cooks distance and Hat values vs studentized residuals , for influence plot.
p1 <- influencePlot(step.model, id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
# AIC with interactions
full.model <- lm(T_box ~ .^2,   data = uvariate_filtered_df  )
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "forward",
trace = TRUE,
steps = 10)
summary(step.model)
summary(step.model)
model_res = resid(step.model)
fitted <- fitted(step.model)
# summary(model_res)
# first view loks fine in terms of normality
hist(model_res, main="Histogram of Residuals",
ylab="Residuals")
plot(fitted, model_res, ylab="Residuals", xlab="BMI Score Fitted", main="BMI Prediction (Residuals)")
#Q-Q Plot of residuals
qqnorm(model_res)
qqline(model_res)
dwtest(step.model) #Test for independence of residuals
# scale numeric data for K means
selected_df_scaled <- selected_df
selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], function(x) c(scale(x)))
# plot the kmeans score per different sets of clusters to attempt to charecterize set,
# use elbow method to evaluate  optimal number of clusters
rng<-2:20 #K from 2 to 20
tries <-100 #Run the K Means algorithm 100 times
avg.totw.ss <-integer(length(rng)) #Set up an empty vector to hold all of points
for(v in rng){ # For each value of the range variable
v.totw.ss <-integer(tries)
for(i in 1:tries){
k.temp <-kmeans(selected_df_scaled,centers=v) #Run kmeans
v.totw.ss[i] <-k.temp$tot.withinss#Store the total withinss
}
avg.totw.ss[v-1] <-mean(v.totw.ss) #Average the 100 total withinss
}
selected_df
dim(df)
dim(selected_df)
dropna(selected_df)
selected_df %>% drop_na()
# scale numeric data for K means
selected_df_scaled <- selected_df
selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], function(x) c(scale(x)))
selected_df_scaled <- selected_df_scaled %>% drop_na()
# plot the kmeans score per different sets of clusters to attempt to charecterize set,
# use elbow method to evaluate  optimal number of clusters
rng<-2:20 #K from 2 to 20
tries <-100 #Run the K Means algorithm 100 times
avg.totw.ss <-integer(length(rng)) #Set up an empty vector to hold all of points
for(v in rng){ # For each value of the range variable
v.totw.ss <-integer(tries)
for(i in 1:tries){
k.temp <-kmeans(selected_df_scaled,centers=v) #Run kmeans
v.totw.ss[i] <-k.temp$tot.withinss#Store the total withinss
}
avg.totw.ss[v-1] <-mean(v.totw.ss) #Average the 100 total withinss
}
plot(rng,avg.totw.ss,type="b", main="Total Within SS by K",
ylab="Average Total Within Sum of Squares",
xlab="Value of K")
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 4),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=4) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- T_box
# scale numeric data for K means
selected_df_scaled <- selected_df
selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], function(x) c(scale(x)))
selected_df_scaled$T_box = T_box
selected_df_scaled <- selected_df_scaled %>% drop_na()
# plot the kmeans score per different sets of clusters to attempt to charecterize set,
# use elbow method to evaluate  optimal number of clusters
rng<-2:15 #K from 2 to 20
tries <-50 #Run the K Means algorithm 100 times
avg.totw.ss <-integer(length(rng)) #Set up an empty vector to hold all of points
for(v in rng){ # For each value of the range variable
v.totw.ss <-integer(tries)
for(i in 1:tries){
k.temp <-kmeans(selected_df_scaled,centers=v) #Run kmeans
v.totw.ss[i] <-k.temp$tot.withinss#Store the total withinss
}
avg.totw.ss[v-1] <-mean(v.totw.ss) #Average the 100 total withinss
}
plot(rng,avg.totw.ss,type="b", main="Total Within SS by K",
ylab="Average Total Within Sum of Squares",
xlab="Value of K")
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 4),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=4) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- selected_df_scaled$T_box
# plot the results
plot(selected_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 3),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=3) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- selected_df_scaled$T_box
# plot the results
plot(selected_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 5),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 5),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=5) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- selected_df_scaled$T_box
# plot the results
plot(selected_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 6),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=6) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- selected_df_scaled$T_box
# plot the results
plot(selected_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 7),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=7) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- selected_df_scaled$T_box
# plot the results
plot(selected_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 4),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=4) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- selected_df_scaled$T_box
# plot the results
plot(selected_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
k.temp$cluster
k.temp$centers
k.temp$size
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 4),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=4) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- selected_df_scaled$T_box
# plot the results
plot(selected_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
# scale numeric data for K means
selected_cols = c('region','urb','income','age','sex.x','race','grade','exercise','kq7','dt01',
'dt02','dt03','dt06','dt07','emp_status')
selected_df <- df %>% dplyr::select(selected_cols)
selected_df_scaled <- selected_df
selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], function(x) c(scale(x)))
selected_df_scaled$T_box = T_box
selected_df_scaled <- selected_df_scaled %>% drop_na()
# plot the kmeans score per different sets of clusters to attempt to charecterize set,
# use elbow method to evaluate  optimal number of clusters
rng<-2:15 #K from 2 to 20
tries <-50 #Run the K Means algorithm 100 times
avg.totw.ss <-integer(length(rng)) #Set up an empty vector to hold all of points
for(v in rng){ # For each value of the range variable
v.totw.ss <-integer(tries)
for(i in 1:tries){
k.temp <-kmeans(selected_df_scaled,centers=v) #Run kmeans
v.totw.ss[i] <-k.temp$tot.withinss#Store the total withinss
}
avg.totw.ss[v-1] <-mean(v.totw.ss) #Average the 100 total withinss
}
plot(rng,avg.totw.ss,type="b", main="Total Within SS by K",
ylab="Average Total Within Sum of Squares",
xlab="Value of K")
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 4),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=4) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- selected_df_scaled$T_box
# plot the results
plot(selected_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
# scale numeric data for K means
selected_cols = c('region','urb','income','age','sex.x','race','grade','exercise','kq7','dt01',
'dt02','dt03','dt06','dt07','emp_status')
selected_df <- df %>% dplyr::select(selected_cols)
selected_df_scaled <- selected_df
selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], function(x) c(scale(x)))
selected_df_scaled$T_box = T_box
selected_df_scaled <- selected_df_scaled %>% drop_na()
# plot the kmeans score per different sets of clusters to attempt to charecterize set,
# use elbow method to evaluate  optimal number of clusters
rng<-2:15 #K from 2 to 20
tries <-100 #Run the K Means algorithm 100 times
avg.totw.ss <-integer(length(rng)) #Set up an empty vector to hold all of points
for(v in rng){ # For each value of the range variable
v.totw.ss <-integer(tries)
for(i in 1:tries){
k.temp <-kmeans(selected_df_scaled,centers=v) #Run kmeans
v.totw.ss[i] <-k.temp$tot.withinss#Store the total withinss
}
avg.totw.ss[v-1] <-mean(v.totw.ss) #Average the 100 total withinss
}
plot(rng,avg.totw.ss,type="b", main="Total Within SS by K",
ylab="Average Total Within Sum of Squares",
xlab="Value of K")
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 4),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=4) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- selected_df_scaled$T_box
# plot the results
plot(selected_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
k.temp.size
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(selected_df_scaled, 4),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(selected_df_scaled,centers=4) #Run kmeans
selected_df_scaled['labels'] <- k.temp$cluster
selected_df_scaled['y'] <- selected_df_scaled$T_box
# plot the results
plot(selected_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
k.temp$size
# print summary of run
#summary(step.model)
#%>% kbl(caption = "Residual Summary") %>%
#  kable_classic(full_width = F, html_font = "Cambria")
# plot the fitted vs actual with a guided regression line for model performance
plot(fitted,df$T_box,col=rgb(0.4,0.4,0.8,0.6),pch=16 , cex=1.3 , main = "BMI ~ Fitted BMI scatter plot")
fitted
dim(fitted)
length(fitted)
length(df$T_box)
# print summary of run
#summary(step.model)
#%>% kbl(caption = "Residual Summary") %>%
#  kable_classic(full_width = F, html_font = "Cambria")
model_res = resid(step.model)
fitted <- fitted(step.model)
# plot the fitted vs actual with a guided regression line for model performance
plot(fitted,df$T_box,col=rgb(0.4,0.4,0.8,0.6),pch=16 , cex=1.3 , main = "BMI ~ Fitted BMI scatter plot")
# Choose columns to filter after univariate runs
#columns_filtered_after_uvariate_regs = #c('region','income','age','sex.x','race','grade','exercise','dt01','dt02','dt03','dt07',
#                                         'emp_status')
columns_filtered_after_uvariate_regs =
c('region','income','age','sex.x','race','grade','exercise','kq7','dt01',
'dt02','dt03','dt07','emp_status',
'kq2_c' , 'kq2_d' , 'kq2_e'    ,'kq2_g' ,   'kq33_a' ,
'kq34' ,  'kq37' ,  'kq38'        ,  'kq42','T_box')
uvariate_filtered_df <- selected_df %>% dplyr::select(columns_filtered_after_uvariate_regs)
# Choose columns to filter after univariate runs
#columns_filtered_after_uvariate_regs = #c('region','income','age','sex.x','race','grade','exercise','dt01','dt02','dt03','dt07',
#                                         'emp_status')
columns_filtered_after_uvariate_regs =
c('region','income','age','sex.x','race','grade','exercise','kq7','dt01',
'dt02','dt03','dt07','emp_status',
'kq2_c' , 'kq2_d' , 'kq2_e'    ,'kq2_g' ,   'kq33_a' ,
'kq34' ,  'kq37' ,  'kq38'        ,  'kq42')
uvariate_filtered_df <- selected_df %>% dplyr::select(columns_filtered_after_uvariate_regs)
selected_cols = c('region','urb','income','age','sex.x','race','grade','exercise','kq7','dt01',
'dt02','dt03','dt06','dt07','emp_status',
'kq2_b'  ,'kq2_c' , 'kq2_d' , 'kq2_e'  ,'kq2_f'  ,'kq2_g' ,   'kq33_a' ,
'kq33_b' ,'kq34' ,  'kq37' ,  'kq38' ,  'kq39',   'kq40'   ,'kq41' ,  'kq42')
selected_df <- df %>% dplyr::select(selected_cols)
##########################################
# Regrress Y vs each column seperately
###########################################
tbl_uv_ex1 <-
tbl_uvregression(
selected_df,
method = glm,
y = T_box,
# exponentiate = TRUE,
pvalue_fun = function(x) style_pvalue(x, digits = 2)
) %>%
add_global_p()
# Choose columns to filter after univariate runs
#columns_filtered_after_uvariate_regs = #c('region','income','age','sex.x','race','grade','exercise','dt01','dt02','dt03','dt07',
#                                         'emp_status')
columns_filtered_after_uvariate_regs =
c('region','income','age','sex.x','race','grade','exercise','kq7','dt01',
'dt02','dt03','dt07','emp_status',
'kq2_c' , 'kq2_d' , 'kq2_e'    ,'kq2_g' ,   'kq33_a' ,
'kq34' ,  'kq37' ,  'kq38'        ,  'kq42')
uvariate_filtered_df <- selected_df %>% dplyr::select(columns_filtered_after_uvariate_regs)
uvariate_filtered_df$T_box = T_box
uvariate_filtered_df<- uvariate_filtered_df %>% drop_na()
# without interactions
full.model <- lm(uvariate_filtered_df$T_box ~., data = uvariate_filtered_df  )
# Stepwise regression model - both ways
step.model <- stepAIC(full.model, direction = "both",
trace = TRUE)
# assign residuals and fitted points to variables
model_res = resid(step.model)
fitted <- fitted(step.model)
# summary(model_res)
#%>% kbl(caption = "Residual Summary") %>%
#  kable_classic(full_width = F, html_font = "Cambria")
# first view loks fine in terms of normality
hist(model_res, main="Histogram of Residuals",
ylab="Residuals")
# plot residuals vs fitted set
plot(fitted, model_res, ylab="Residuals", xlab="BMI Score Fitted", main="BMI Prediction (residuals)")
#Q-Q Plot of residuals
qqnorm(model_res)
qqline(model_res)
# AIC with interactions
full.model <- lm(uvariate_filtered_df$T_box ~ .^2,   data = uvariate_filtered_df  )
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "forward",
trace = TRUE,
steps = 10)
summary(step.model)
# print summary of run
#summary(step.model)
#%>% kbl(caption = "Residual Summary") %>%
#  kable_classic(full_width = F, html_font = "Cambria")
model_res = resid(step.model)
fitted <- fitted(step.model)
# plot the fitted vs actual with a guided regression line for model performance
plot(fitted,df$T_box,col=rgb(0.4,0.4,0.8,0.6),pch=16 , cex=1.3 , main = "BMI ~ Fitted BMI scatter plot")
plot(fitted,uvariate_filtered_df$T_box,col=rgb(0.4,0.4,0.8,0.6),pch=16 , cex=1.3 , main = "BMI ~ Fitted BMI scatter plot")
# print summary of run
#summary(step.model)
#%>% kbl(caption = "Residual Summary") %>%
#  kable_classic(full_width = F, html_font = "Cambria")
model_res = resid(step.model)
fitted <- fitted(step.model)
# plot the fitted vs actual with a guided regression line for model performance
plot(fitted,uvariate_filtered_df$T_box,col=rgb(0.4,0.4,0.8,0.6),pch=16 , cex=1.3 , main = "BMI ~ Fitted BMI scatter plot")
model <- lm(df$T_box ~ fitted + I(fitted^2) )
# print summary of run
#summary(step.model)
#%>% kbl(caption = "Residual Summary") %>%
#  kable_classic(full_width = F, html_font = "Cambria")
model_res = resid(step.model)
fitted <- fitted(step.model)
# plot the fitted vs actual with a guided regression line for model performance
plot(fitted,uvariate_filtered_df$T_box,col=rgb(0.4,0.4,0.8,0.6),pch=16 , cex=1.3 , main = "BMI ~ Fitted BMI scatter plot")
model <- lm(uvariate_filtered_df$T_box ~ fitted + I(fitted^2) )
myPredict <- predict( model )
ix <- sort(fitted,index.return=T)$ix
lines(fitted[ix], myPredict[ix], col=2, lwd=2 )
model_res = resid(step.model)
fitted <- fitted(step.model)
# summary(model_res)
# first view loks fine in terms of normality
hist(model_res, main="Histogram of Residuals",
ylab="Residuals")
plot(fitted, model_res, ylab="Residuals", xlab="BMI Score Fitted", main="BMI Prediction (Residuals)")
#Q-Q Plot of residuals
qqnorm(model_res)
qqline(model_res)
# scale numeric data for K means
s_cols = c('region','urb','income','age','sex.x','race','grade','exercise','kq7','dt01',
'dt02','dt03','dt06','dt07','emp_status')
s_df <- df %>% dplyr::select(s_cols)
s_df_scaled <- s_df
s_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], function(x) c(scale(x)))
# scale numeric data for K means
s_cols = c('region','urb','income','age','sex.x','race','grade','exercise','kq7','dt01',
'dt02','dt03','dt06','dt07','emp_status')
s_df <- df %>% dplyr::select(s_cols)
s_df_scaled <- s_df
s_df_scaled[c('age','income','grade')] <- lapply(s_df_scaled[c('age','income')], function(x) c(scale(x)))
s_df_scaled$T_box = T_box
s_df_scaled <- s_df_scaled %>% drop_na()
# plot the kmeans score per different sets of clusters to attempt to charecterize set,
# use elbow method to evaluate  optimal number of clusters
rng<-2:15 #K from 2 to 20
tries <-100 #Run the K Means algorithm 100 times
avg.totw.ss <-integer(length(rng)) #Set up an empty vector to hold all of points
for(v in rng){ # For each value of the range variable
v.totw.ss <-integer(tries)
for(i in 1:tries){
k.temp <-kmeans(s_df_scaled,centers=v) #Run kmeans
v.totw.ss[i] <-k.temp$tot.withinss#Store the total withinss
}
avg.totw.ss[v-1] <-mean(v.totw.ss) #Average the 100 total withinss
}
plot(rng,avg.totw.ss,type="b", main="Total Within SS by K",
ylab="Average Total Within Sum of Squares",
xlab="Value of K")
# scale numeric data for K means
#selected_df_scaled <- selected_df
#selected_df_scaled[c('age','income','grade')] <- lapply(selected_df_scaled[c('age','income')], #function(x) c(scale(x)))
#plot pca after kmeans
autoplot(clara(s_df_scaled, 4),frame=TRUE, main='Plot of clusters after PCA dimension reduction')
# perform kmeans and assign each observation its label
k.temp <-kmeans(s_df_scaled,centers=4) #Run kmeans
s_df_scaled['labels'] <- k.temp$cluster
s_df_scaled['y'] <- s_df_scaled$T_box
# plot the results
plot(s_df_scaled %>% group_by(labels) %>%   summarize(mean_size = mean(y, na.rm = TRUE)),
main =" 4 Cluster BMI Means")
print(tbl_uv_ex1)
tbl_uv_ex1
summary(tbl_uv_ex1)
tbl_uv_ex1.as_flex_table()
tbl_uv_ex1 <-
tbl_uvregression(
selected_df,
method = glm,
y = T_box,
show_single_row = TRUE
# exponentiate = TRUE,
pvalue_fun = function(x) style_pvalue(x, digits = 2)
tbl_uv_ex1 <-
tbl_uvregression(
selected_df,
method = glm,
y = T_box,
show_single_row = TRUE,
# exponentiate = TRUE,
pvalue_fun = function(x) style_pvalue(x, digits = 2)
) %>%
add_global_p()
tbl_uv_ex1 <-
tbl_uvregression(
selected_df,
method = glm,
y = T_box,
show_single_row = selected_cols,
# exponentiate = TRUE,
pvalue_fun = function(x) style_pvalue(x, digits = 2)
) %>%
add_global_p()
dim(df)
tbl_uv_ex1 %>% kbl(caption = "Univariate Results TransBMI ~ X") %>%
kable_classic(full_width = F, html_font = "Cambria")
knitr::opts_chunk$set(echo = FALSE)
library(haven)
dataset
path = file.path( "ETT_ESM_Study1.sav")
dataset = read_sav(path)
dataset
df = read_sav(path)
summary(df)
df
df
plot(df$country)
plot(df$DAY)
cols(df)
columns(df)
unlink('School/courses/applied_stats/p2/project2_cache', recursive = TRUE)
install.packages('tinytex')
install.packages("tinytex")
render("input.Rmd", "pdf_document")
knitr::opts_chunk$set(echo = FALSE)
setwd("School/courses/applied_stats/p2")
knit_with_parameters('~/School/courses/applied_stats/p2/project2.Rmd')
